{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db5c362-8da3-4d12-9230-fd2613e14863",
   "metadata": {},
   "source": [
    "# Training the YOLOv8 Model (RGB / IR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e82a2-00e4-4a99-8bad-dd2c988516cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# missing yolo dep\n",
    "!pip install lapx>=0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd5933f-ad78-4c84-88c8-37b908a5b85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pride/work/AIMET/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f57f-1fe8-44d3-b45e-dfb4356d3333",
   "metadata": {},
   "source": [
    "## YOLOv8 Nano (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e47ef7f-ac0d-4d78-a595-ddeae14032de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unzip downloaded dataset to `./datasets`\n",
    "#dataset_rgb = 'datasets/data_rgb.yaml'\n",
    "dataset_rgb = 'config/data_rgb.yaml'\n",
    "\n",
    "# load a model\n",
    "backbone_nano = YOLO(\"yolov8n.yaml\")  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a8a09-c437-4428-b18d-a74a3b53d15b",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6b26f8-739b-4a7e-8db1-f346ca335879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "#f = open('./datasets/images_rgb_train/coco.json')\n",
    "f = open('./config/images_rgb_train_coco.json')\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfbc422-7014-47d8-b0c6-126c71c9db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the json\n",
    "list = []\n",
    "for i in data['annotations']:\n",
    "    list.append(i['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c4cf80-a3f0-4a63-ba39-dd15b6fdd5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e731bb3-7915-4276-85ff-27887e8345e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  5  6  8  9 12 13 14 15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.unique(list))\n",
    "\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e976178-5ce0-4485-a3a4-65c9d6c5f858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.101 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.193 🚀 Python-3.10.12 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24195MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=config/data_rgb.yaml, epochs=1, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train7\n",
      "Overriding model.yaml nc=80 with nc=16\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    754432  ultralytics.nn.modules.head.Detect           [16, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3013968 parameters, 3013952 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/pride/work/AIMET/yolov8-nightshift/datasets/images_rgb_train/labels.cache... 10318 images, 266 backgrounds, 0 corrupt: 100%|██████████| 10319/10319 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pride/work/AIMET/yolov8-nightshift/datasets/images_rgb_val/labels.cache... 1085 images, 16 backgrounds, 0 corrupt: 100%|██████████| 1085/1085 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train7/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train7\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/1       3.1G      3.886      4.335      3.427        256        640: 100%|██████████| 645/645 [00:51<00:00, 12.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:03<00:00,  9.88it/s]\n",
      "                   all       1085      16909      0.743     0.0143    0.00943    0.00337\n",
      "\n",
      "1 epochs completed in 0.016 hours.\n",
      "Optimizer stripped from runs/detect/train7/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train7/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train7/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.193 🚀 Python-3.10.12 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24195MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3008768 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:03<00:00, 11.12it/s]\n",
      "                   all       1085      16909      0.743     0.0143    0.00939    0.00336\n",
      "                person       1085       3223          1          0    0.00154   0.000402\n",
      "                  bike       1085        193          1          0   0.000303   7.35e-05\n",
      "                   car       1085       7285      0.175      0.157     0.0871     0.0305\n",
      "                 motor       1085         77          1          0          0          0\n",
      "                 train       1085        183          1          0   0.000529   9.42e-05\n",
      "                 truck       1085       2190          1          0     0.0123    0.00566\n",
      "               hydrant       1085        126          0          0          0          0\n",
      "                  sign       1085       3581          1          0     0.0015    0.00015\n",
      "            skateboard       1085          4          1          0          0          0\n",
      "              stroller       1085          7          0          0          0          0\n",
      "         other vehicle       1085         40          1          0          0          0\n",
      "Speed: 0.2ms preprocess, 0.6ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results_n = backbone_nano.train(data=dataset_rgb, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954601e8-f58f-46fe-b78c-0d4b6d168515",
   "metadata": {},
   "source": [
    "> 20 epochs completed in 1.521 hours.\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 10/20 | 3.07G | 1.829 | 1.375 | 1.254 | 328 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1085 | 16909 | 0.525 | 0.16 | 0.156 | 0.077 |\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 20/20 | 2.63G | 1.595 | 1.117 | 1.146 | 223 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1085 | 16909 | 0.579 | 0.185 | 0.196 | 0.102 |\n",
    "\n",
    "\n",
    "> YOLOv8n summary (fused): 168 layers, 3008768 parameters, 0 gradients\n",
    "\n",
    "| Class | Images | Instances | P | R |  mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1085 | 16909 | 0.578 | 0.186 | 0.196 | 0.102 |\n",
    "| person | 1085 | 3223 | 0.501 | 0.375 | 0.389 | 0.167 |\n",
    "| bike | 1085 | 193 | 0.201 | 0.197 | 0.101 | 0.0438 |\n",
    "| car | 1085 | 7285 | 0.662 | 0.575 | 0.621 | 0.397 |\n",
    "| motor | 1085 | 77 | 0.418 | 0.26 | 0.298 | 0.164 |\n",
    "| train | 1085 | 183 | 0.458 | 0.246 | 0.253 | 0.153 |\n",
    "| truck | 1085 | 2190 | 0.458 | 0.198 | 0.206 | 0.0686 |\n",
    "| hydrant | 1085 | 126 | 0.744 | 0.0232 | 0.0797 | 0.0265 |\n",
    "| sign | 1085 | 3581 | 0.564 | 0.143 | 0.171 | 0.0824 |\n",
    "| skateboard | 1085 | 4 | 1 | 0 | 0 | 0 |\n",
    "| stroller | 1085 | 7 | 1 | 0 | 0.018 | 0.0144 |\n",
    "| other vehicle | 1085 | 40 | 0.348 | 0.025 | 0.0231 | 0.00793 |\n",
    "\n",
    "\n",
    "> _Speed: 0.2ms preprocess, 4.2ms inference, 0.0ms loss, 0.6ms postprocess per image_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9f41b-bf90-4413-b8e1-f27a9c8e8403",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_rgb_nano_results.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05b34d-c191-4770-a375-6f37102900ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 14))\n",
    "\n",
    "im_batch0_labels = plt.imread('./assets/backbone_rgb_nano_val_batch0_labels.webp')\n",
    "im_batch0_pred = plt.imread('./assets/backbone_rgb_nano_val_batch0_pred.webp')\n",
    "im_batch1_labels = plt.imread('./assets/backbone_rgb_nano_val_batch1_labels.webp')\n",
    "im_batch1_pred = plt.imread('./assets/backbone_rgb_nano_val_batch1_pred.webp')\n",
    "im_batch2_labels = plt.imread('./assets/backbone_rgb_nano_val_batch2_labels.webp')\n",
    "im_batch2_pred = plt.imread('./assets/backbone_rgb_nano_val_batch2_pred.webp')\n",
    "\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "plt.title('batch0_labels')\n",
    "plt.imshow(im_batch0_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 2)\n",
    "plt.title('batch1_labels')\n",
    "plt.imshow(im_batch1_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 3)\n",
    "plt.title('batch2_labels')\n",
    "plt.imshow(im_batch2_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 4)\n",
    "plt.title('batch0_pred')\n",
    "plt.imshow(im_batch0_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 5)\n",
    "plt.title('batch1_pred')\n",
    "plt.imshow(im_batch1_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 6)\n",
    "plt.title('batch2_pred')\n",
    "plt.imshow(im_batch2_pred)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac93999-c08e-46e9-b9e2-27413961635f",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98de72-ba08-4c0b-b696-fa95d3b22ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "results_n = backbone_nano.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d0eab-904e-4cae-a708-65b3124c13ea",
   "metadata": {},
   "source": [
    "| Class | Images | Instances | P | R |  mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1085 | 16909 | 0.578 | 0.185 | 0.198 | 0.104 |\n",
    "| person | 1085 | 3223 | 0.505 | 0.375 | 0.391 | 0.167 |\n",
    "| bike | 1085 | 193 | 0.2 | 0.197 | 0.102 | 0.044 |\n",
    "| car | 1085 | 7285 | 0.663 | 0.574 | 0.621 | 0.398 |\n",
    "| motor | 1085 | 77 | 0.419 | 0.26 | 0.3 | 0.166 |\n",
    "| train | 1085 | 183 | 0.455 | 0.246 | 0.252 | 0.155 |\n",
    "| truck | 1085 | 2190 | 0.458 | 0.197 | 0.205 | 0.0686 |\n",
    "| hydrant | 1085 | 126 | 0.741 | 0.023 | 0.0801 | 0.0274 |\n",
    "| sign | 1085 | 3581 | 0.563 | 0.142 | 0.17 | 0.0824 |\n",
    "| skateboard | 1085 | 4 | 1 | 0 | 0 | 0 |\n",
    "| stroller | 1085 | 7 | 1 | 0 | 0.0353 | 0.023 |\n",
    "| other vehicle | 1085 | 40 | 0.355 | 0.025 | 0.0231 | 0.00793 |\n",
    "\n",
    "> _Speed: 0.3ms preprocess, 5.0ms inference, 0.0ms loss, 0.7ms postprocess per image_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa619094-cac5-4073-bc75-4897020ef4bb",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_rgb_nano_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e883ea-ba0c-4f0c-be41-efd9e89bf36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX format\n",
    "# success = backbone_nano.export(imgsz=(640, 480), format='onnx', opset=12, optimize=False, half=False)\n",
    "# Export to PyTorch format\n",
    "success = backbone_nano.export(imgsz=640, format='torchscript', optimize=False, half=False, int8=False)\n",
    "# TorchScript: export success ✅ 1.3s, saved as 'runs/detect/train4/weights/best.torchscript' (11.9 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad3fff-166b-4354-bc3e-f6055a9092f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick pre-trained model\n",
    "n_model = YOLO('runs/detect/train6/weights/best.torchscript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb94a3-0eb5-449b-b06a-70052fb4e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read video by index\n",
    "video = cv.VideoCapture(videos[1])\n",
    "ret, frame = video.read()\n",
    "\n",
    "# get video dims\n",
    "frame_width = int(video.get(3))\n",
    "frame_height = int(video.get(4))\n",
    "size = (frame_width, frame_height)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv.VideoWriter('./outputs/backbone_nano_rgb.avi', fourcc, 20.0, size)\n",
    "\n",
    "# read frames\n",
    "ret = True\n",
    "\n",
    "while ret:\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        # detect & track objects\n",
    "        results = np_model.track(frame, persist=True)\n",
    "\n",
    "        # plot results\n",
    "        composed = results[0].plot()\n",
    "\n",
    "        # save video\n",
    "        out.write(composed)\n",
    "\n",
    "out.release()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab4160-2731-4ff1-9fb2-7131e4887c6d",
   "metadata": {},
   "source": [
    "## YOLOv8 Small (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cbe63-8954-4cf1-bc2a-9c301a5923e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip downloaded dataset to `./datasets`\n",
    "dataset_rgb = 'datasets/data_rgb.yaml'\n",
    "\n",
    "# load a model\n",
    "backbone_small = YOLO(\"yolov8s.yaml\")  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c5380-d458-4d37-9ba5-0d094e255616",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b94d15-28e9-4c0d-b724-1d3346a5fe1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results_s = backbone_small.train(data=dataset_rgb, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981849a-26a3-45b0-83a0-42735661ed07",
   "metadata": {},
   "source": [
    "> 20 epochs completed in 2.438 hours.\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 10/20 | 4.84G | 1.569 | 1.098 | 1.195 | 328 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1085 | 16909 | 0.596 | 0.211 | 0.245 | 0.128 |\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 20/20 | 4.67G | 1.367 | 0.8879 | 1.083 | 223 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1085 | 16909 | 0.608 | 0.25 | 0.291 | 0.158 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> YOLOv8s summary (fused): 168 layers, 11131776 parameters, 0 gradients\n",
    "\n",
    "| Class | Images | Instances | P | R |  mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1085 | 16909 | 0.523 | 0.255 | 0.291 | 0.159 |\n",
    "| person | 1085 | 3223 | 0.618 | 0.428 | 0.481 | 0.225 |\n",
    "| bike | 1085 | 193 | 0.248 | 0.326 | 0.239 | 0.121 |\n",
    "| car | 1085 | 7285 | 0.718 | 0.63 | 0.683 | 0.454 |\n",
    "| motor | 1085 | 77 | 0.566 | 0.338 | 0.382 | 0.22 |\n",
    "| train | 1085 | 183 | 0.577 | 0.344 | 0.409 | 0.276 |\n",
    "| truck | 1085 | 2190 | 0.593 | 0.318 | 0.336 | 0.119 |\n",
    "| hydrant | 1085 | 126 | 0.8 | 0.175 | 0.293 | 0.129 |\n",
    "| sign | 1085 | 3581 | 0.632 | 0.243 | 0.291 | 0.149 |\n",
    "| skateboard | 1085 | 4 | 0 | 0 | 0 | 0 |\n",
    "| stroller | 1085 | 7 | 1 | 0 | 0.0687 | 0.0477 |\n",
    "| other vehicle | 1085 | 40 | 0 | 0 | 0.0135 | 0.00526 |\n",
    "\n",
    "> _Speed: 0.3ms preprocess, 9.6ms inference, 0.0ms loss, 0.5ms postprocess per image_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0214c-c2e9-4bd5-8c98-02eb6d0c0455",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_rgb_small_results.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df4576-1600-41c4-8969-6d367b92c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 14))\n",
    "\n",
    "im_batch0_labels = plt.imread('./assets/backbone_rgb_small_val_batch0_labels.webp')\n",
    "im_batch0_pred = plt.imread('./assets/backbone_rgb_small_val_batch0_pred.webp')\n",
    "im_batch1_labels = plt.imread('./assets/backbone_rgb_small_val_batch1_labels.webp')\n",
    "im_batch1_pred = plt.imread('./assets/backbone_rgb_small_val_batch1_pred.webp')\n",
    "im_batch2_labels = plt.imread('./assets/backbone_rgb_small_val_batch2_labels.webp')\n",
    "im_batch2_pred = plt.imread('./assets/backbone_rgb_small_val_batch2_pred.webp')\n",
    "\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "plt.title('batch0_labels')\n",
    "plt.imshow(im_batch0_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 2)\n",
    "plt.title('batch1_labels')\n",
    "plt.imshow(im_batch1_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 3)\n",
    "plt.title('batch2_labels')\n",
    "plt.imshow(im_batch2_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 4)\n",
    "plt.title('batch0_pred')\n",
    "plt.imshow(im_batch0_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 5)\n",
    "plt.title('batch1_pred')\n",
    "plt.imshow(im_batch1_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 6)\n",
    "plt.title('batch2_pred')\n",
    "plt.imshow(im_batch2_pred)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57450a33-3cff-4abf-be65-acf70e693ecc",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde064e4-9166-4ea1-85e4-94d2a85b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "results_s = backbone_small.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ca3cf-7a32-4e9a-8024-cc2eeb04b3bd",
   "metadata": {},
   "source": [
    "| Class | Images | Instances | P | R |  mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1085 | 16909 | 0.524 | 0.253 | 0.29 | 0.159 |\n",
    "| person | 1085 | 3223 | 0.622 | 0.428 | 0.481 | 0.225 |\n",
    "| bike | 1085 | 193 | 0.247 | 0.321 | 0.239 | 0.121 |\n",
    "| car | 1085 | 7285 | 0.722 | 0.629 | 0.683 | 0.454 |\n",
    "| motor | 1085 | 77 | 0.563 | 0.338 | 0.382 | 0.219 |\n",
    "| train | 1085 | 183 | 0.575 | 0.339 | 0.41 | 0.276 |\n",
    "| truck | 1085 | 2190 | 0.6 | 0.315 | 0.333 | 0.12 |\n",
    "| hydrant | 1085 | 126 | 0.8 | 0.175 | 0.292 | 0.129 |\n",
    "| sign | 1085 | 3581 | 0.635 | 0.243 | 0.292 | 0.149 |\n",
    "| skateboard | 1085 | 4 | 0 | 0 | 0 | 0 |\n",
    "| stroller | 1085 | 7 | 1 | 0 | 0.069 | 0.0479 |\n",
    "| other vehicle | 1085 | 40 | 0 | 0 | 0.0136 | 0.00526 |\n",
    "\n",
    "> _Speed: 0.4ms preprocess, 10.9ms inference, 0.0ms loss, 0.6ms postprocess per image_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25e05c-60ec-488a-9af8-016b7c0bd3c7",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_rgb_small_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693f0c1-d7d1-44b5-bfc0-e86e41520af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX format\n",
    "# success = backbone_nano.export(imgsz=(640, 480), format='onnx', opset=12, optimize=False, half=False)\n",
    "# Export to PyTorch format\n",
    "success = backbone_small.export(imgsz=640, format='torchscript', optimize=False, half=False, int8=False)\n",
    "TorchScript: export success ✅ 2.1s, saved as 'runs/detect/train5/weights/best.torchscript' (42.9 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8177fd-1288-4e35-af02-48657725e275",
   "metadata": {},
   "source": [
    "## YOLOv8 Nano (IR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f765e6-8070-4455-a616-71bbad163a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip downloaded dataset to `./datasets`\n",
    "dataset_ir = 'datasets/data_thermal.yaml'\n",
    "\n",
    "# load a model\n",
    "backbone_ir_nano = YOLO(\"yolov8n.yaml\")  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96a6d2-e83b-4056-b275-106d6118cd0f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79947433-8e80-4a81-8da8-34043764174c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results_ir_n = backbone_ir_nano.train(data=dataset_ir, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce681d2-dea3-45d1-a5f5-848cddbb0b99",
   "metadata": {},
   "source": [
    "> 20 epochs completed in 1.337 hours.\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 10/20 | 3.33G | 1.746 | 1.263 | 1.211 | 104 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1144 | 16688 | 0.466 | 0.186 | 0.226 | 0.112 |\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 20/20 | 2.5G | 1.518 | 1.016 | 1.111 | 102 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1144 | 16688 | 0.513 | 0.249 | 0.276 | 0.146 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> YOLOv8n summary (fused): 168 layers, 3008768 parameters, 0 gradients\n",
    "\n",
    "| Class | Images | Instances | P | R |  mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1144 | 16688 | 0.514 | 0.249 | 0.276 | 0.146 |\n",
    "| person | 1144 | 4470 | 0.628 | 0.555 | 0.594 | 0.276 |\n",
    "| bike | 1144 | 170 | 0.278 | 0.25 | 0.219 | 0.11 |\n",
    "| car | 1144 | 7128 | 0.691 | 0.65 | 0.71 | 0.449 |\n",
    "| motor | 1144 | 55 | 0.569 | 0.364 | 0.39 | 0.19 |\n",
    "| train | 1144 | 179 | 0.741 | 0.383 | 0.455 | 0.284 |\n",
    "| truck | 1144 | 2048 | 0.467 | 0.259 | 0.274 | 0.105 |\n",
    "| hydrant | 1144 | 94 | 0.678 | 0.0638 | 0.12 | 0.0535 |\n",
    "| sign | 1144 | 2472 | 0.557 | 0.2 | 0.255 | 0.132 |\n",
    "| skateboard | 1144 | 3 | 0 | 0 | 0 | 0 |\n",
    "| stroller | 1144 | 6 | 1 | 0 | 0 | 0 |\n",
    "| other vehicle | 1144 | 63 | 0.0423 | 0.0159 | 0.0194 | 0.00652 |\n",
    "\n",
    "> _Speed: 0.3ms preprocess, 3.8ms inference, 0.0ms loss, 0.6ms postprocess per image_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3862cbd-7fc6-4385-bbca-a1bb0d499ce3",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_ir_nano_results.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c062d-709a-4a32-bd09-950205ea8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 14))\n",
    "\n",
    "im_batch0_labels = plt.imread('./assets/backbone_ir_nano_val_batch0_labels.webp')\n",
    "im_batch0_pred = plt.imread('./assets/backbone_ir_nano_val_batch0_pred.webp')\n",
    "im_batch1_labels = plt.imread('./assets/backbone_ir_nano_val_batch1_labels.webp')\n",
    "im_batch1_pred = plt.imread('./assets/backbone_ir_nano_val_batch1_pred.webp')\n",
    "im_batch2_labels = plt.imread('./assets/backbone_ir_nano_val_batch2_labels.webp')\n",
    "im_batch2_pred = plt.imread('./assets/backbone_ir_nano_val_batch2_pred.webp')\n",
    "\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "plt.title('batch0_labels')\n",
    "plt.imshow(im_batch0_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 2)\n",
    "plt.title('batch1_labels')\n",
    "plt.imshow(im_batch1_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 3)\n",
    "plt.title('batch2_labels')\n",
    "plt.imshow(im_batch2_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 4)\n",
    "plt.title('batch0_pred')\n",
    "plt.imshow(im_batch0_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 5)\n",
    "plt.title('batch1_pred')\n",
    "plt.imshow(im_batch1_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 6)\n",
    "plt.title('batch2_pred')\n",
    "plt.imshow(im_batch2_pred)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd347e35-870c-4191-b776-f1db1adbacca",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2a964-b929-4116-8a5b-1425a2534a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "results_ir_n = backbone_ir_nano.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcaabd6-1724-451d-9bd3-664398ba1cd6",
   "metadata": {},
   "source": [
    "| Class | Images | Instances | P | R |  mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1144 | 16688 | 0.516 | 0.249 | 0.276 | 0.146 |\n",
    "| person | 1144 | 4470 | 0.631 | 0.556 | 0.595 | 0.276 |\n",
    "| bike | 1144 | 170 | 0.288 | 0.253 | 0.222 | 0.111 |\n",
    "| car | 1144 | 7128 | 0.696 | 0.65 | 0.711 | 0.449 |\n",
    "| motor | 1144 | 55 | 0.57 | 0.364 | 0.39 | 0.189 |\n",
    "| train | 1144 | 179 | 0.746 | 0.378 | 0.455 | 0.283 |\n",
    "| truck | 1144 | 2048 | 0.462 | 0.256 | 0.271 | 0.104 |\n",
    "| hydrant | 1144 | 94 | 0.679 | 0.0638 | 0.12 | 0.0526 |\n",
    "| sign | 1144 | 2472 | 0.557 | 0.199 | 0.256 | 0.132 |\n",
    "| skateboard | 1144 | 3 | 0 | 0 | 0 | 0 |\n",
    "| stroller | 1144 | 6 | 1 | 0 | 0 | 0 |\n",
    "| other vehicle | 1144 | 63 | 0.0425 | 0.0159 | 0.0193 | 0.00637 |\n",
    "\n",
    "> _Speed: 0.3ms preprocess, 4.5ms inference, 0.0ms loss, 0.7ms postprocess per image_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37577d8-b911-49a0-916f-927831e235e4",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_ir_nano_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64814c-2800-4640-bcc9-ed9332fca0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX format\n",
    "# success = backbone_nano.export(imgsz=(640, 480), format='onnx', opset=12, optimize=False, half=False)\n",
    "# Export to PyTorch format\n",
    "success = backbone_ir_nano.export(imgsz=640, format='torchscript', optimize=False, half=False, int8=False)\n",
    "# TorchScript: export success ✅ 1.6s, saved as 'runs/detect/train6/weights/best.torchscript' (12.4 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed6660-e9cc-4303-9ce5-8849561e437e",
   "metadata": {},
   "source": [
    "## YOLOv8 Small (IR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76501702-222b-40ee-b612-66ee0b6471bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2147008  ultralytics.nn.modules.head.Detect           [80, [128, 256, 512]]         \n",
      "YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients, 28.8 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unzip downloaded dataset to `./datasets`\n",
    "#dataset_ir = 'datasets/data_thermal.yaml'\n",
    "dataset_ir = 'config/data_thermal.yaml'\n",
    "\n",
    "# load a model\n",
    "backbone_ir_small = YOLO(\"yolov8s.yaml\")  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef4246-3443-4063-8ae9-3971e70bc10c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1079eff-88fd-4d19-9dd4-de92db5bd057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.101 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.193 🚀 Python-3.10.12 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24195MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=config/data_thermal.yaml, epochs=20, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train8\n",
      "Overriding model.yaml nc=80 with nc=16\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2122240  ultralytics.nn.modules.head.Detect           [16, [128, 256, 512]]         \n",
      "YOLOv8s summary: 225 layers, 11141792 parameters, 11141776 gradients, 28.7 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/pride/work/AIMET/yolov8-nightshift/datasets/images_rgb_train/labels.cache... 10318 images, 266 backgrounds, 0 corrupt: 100%|██████████| 10319/10319 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pride/work/AIMET/yolov8-nightshift/datasets/images_rgb_val/labels.cache... 1085 images, 16 backgrounds, 0 corrupt: 100%|██████████| 1085/1085 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train8\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20      5.27G       3.62      3.767      3.214        256        640: 100%|██████████| 645/645 [01:11<00:00,  8.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.62it/s]\n",
      "                   all       1085      16909      0.502     0.0314     0.0283      0.011\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20      5.37G      2.531      2.117      1.834        286        640: 100%|██████████| 645/645 [01:08<00:00,  9.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.50it/s]\n",
      "                   all       1085      16909      0.238     0.0862     0.0696     0.0316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20      5.38G      2.196      1.747      1.557        461        640: 100%|██████████| 645/645 [01:08<00:00,  9.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.63it/s]\n",
      "                   all       1085      16909      0.378      0.103     0.0932     0.0441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20      5.37G      1.992      1.532      1.427        432        640: 100%|██████████| 645/645 [01:07<00:00,  9.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.59it/s]\n",
      "                   all       1085      16909      0.428      0.144       0.14     0.0687\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20      5.37G      1.868        1.4      1.353        299        640: 100%|██████████| 645/645 [01:07<00:00,  9.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.56it/s]\n",
      "                   all       1085      16909      0.514      0.159      0.159     0.0773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20      5.37G      1.772      1.297      1.297        290        640: 100%|██████████| 645/645 [01:08<00:00,  9.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.86it/s]\n",
      "                   all       1085      16909      0.642      0.169      0.183     0.0864\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20      5.37G      1.698      1.224      1.258        296        640: 100%|██████████| 645/645 [01:07<00:00,  9.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.61it/s]\n",
      "                   all       1085      16909      0.654      0.193      0.209      0.104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20      5.37G       1.65      1.178      1.229        261        640: 100%|██████████| 645/645 [01:07<00:00,  9.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.70it/s]\n",
      "                   all       1085      16909      0.629      0.203      0.215      0.109\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20      5.37G      1.607      1.135      1.204        318        640: 100%|██████████| 645/645 [01:08<00:00,  9.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.65it/s]\n",
      "                   all       1085      16909      0.704      0.195       0.23      0.116\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20      5.37G      1.581      1.108      1.186        328        640: 100%|██████████| 645/645 [01:07<00:00,  9.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.71it/s]\n",
      "                   all       1085      16909      0.572      0.222      0.253      0.131\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20      5.37G       1.53       1.06      1.166        202        640: 100%|██████████| 645/645 [01:07<00:00,  9.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.74it/s]\n",
      "                   all       1085      16909      0.482      0.222      0.243      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20      5.37G      1.502      1.027      1.152        213        640: 100%|██████████| 645/645 [01:06<00:00,  9.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.83it/s]\n",
      "                   all       1085      16909      0.629      0.218      0.256      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      5.37G      1.479      1.003      1.135        192        640: 100%|██████████| 645/645 [01:06<00:00,  9.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 11.91it/s]\n",
      "                   all       1085      16909      0.566      0.248      0.263      0.136\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20      5.37G      1.457     0.9836      1.126        177        640: 100%|██████████| 645/645 [01:05<00:00,  9.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.12it/s]\n",
      "                   all       1085      16909       0.46      0.257      0.266      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20      5.37G      1.435     0.9602      1.113        211        640: 100%|██████████| 645/645 [01:05<00:00,  9.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.15it/s]\n",
      "                   all       1085      16909      0.513      0.252      0.271      0.144\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20      5.37G       1.42     0.9444      1.106        193        640: 100%|██████████| 645/645 [01:05<00:00,  9.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.18it/s]\n",
      "                   all       1085      16909      0.543      0.259      0.272      0.144\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20      5.37G      1.401     0.9259      1.097        274        640: 100%|██████████| 645/645 [01:05<00:00,  9.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.12it/s]\n",
      "                   all       1085      16909       0.56      0.259      0.285      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20      5.37G      1.393     0.9138      1.091        240        640: 100%|██████████| 645/645 [01:05<00:00,  9.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.17it/s]\n",
      "                   all       1085      16909      0.527      0.243      0.283      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20      5.37G       1.38     0.9009      1.084        237        640: 100%|██████████| 645/645 [01:05<00:00,  9.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.11it/s]\n",
      "                   all       1085      16909      0.606      0.257      0.289      0.154\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20      5.37G       1.37     0.8918      1.079        223        640: 100%|██████████| 645/645 [01:05<00:00,  9.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:02<00:00, 12.20it/s]\n",
      "                   all       1085      16909      0.524      0.269      0.287      0.153\n",
      "\n",
      "20 epochs completed in 0.392 hours.\n",
      "Optimizer stripped from runs/detect/train8/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/detect/train8/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/detect/train8/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.193 🚀 Python-3.10.12 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24195MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11131776 parameters, 0 gradients, 28.5 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 34/34 [00:04<00:00,  8.06it/s]\n",
      "                   all       1085      16909      0.603      0.257      0.289      0.153\n",
      "                person       1085       3223      0.616       0.43      0.488      0.229\n",
      "                  bike       1085        193       0.26      0.311      0.215      0.105\n",
      "                   car       1085       7285      0.707      0.644       0.69      0.458\n",
      "                 motor       1085         77      0.611      0.442      0.456      0.229\n",
      "                 train       1085        183      0.596      0.339      0.418      0.281\n",
      "                 truck       1085       2190      0.584      0.304      0.332      0.122\n",
      "               hydrant       1085        126      0.622      0.118      0.252     0.0983\n",
      "                  sign       1085       3581      0.632      0.238      0.278      0.145\n",
      "            skateboard       1085          4          0          0          0          0\n",
      "              stroller       1085          7          1          0     0.0206     0.0108\n",
      "         other vehicle       1085         40          1          0     0.0289    0.00902\n",
      "Speed: 0.2ms preprocess, 1.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results_ir_s = backbone_ir_small.train(data=dataset_ir, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e6be2-c8cf-4b57-ab55-066c3832c269",
   "metadata": {},
   "source": [
    "> 20 epochs completed in 2.827 hours.\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 10/20 | 4.83G | 1.508 | 1.018 | 1.16 | 104 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1144 | 16688 | 0.489 | 0.286 | 0.313 | 0.168 |\n",
    "\n",
    "| Epoch | GPU_mem | box_loss | cls_loss | dfl_loss | Instances | Size |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| 20/20 | 4.67G | 1.317 | 0.8207 | 1.064 | 102 |\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| all | 1144 | 16688 | 0.554 | 0.322 | 0.358 | 0.2 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> YOLOv8s summary (fused): 168 layers, 11131776 parameters, 0 gradients\n",
    "\n",
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1144 | 16688 | 0.554 | 0.322 | 0.358 | 0.2 |\n",
    "| person | 1144 | 4470 | 0.687 | 0.634 | 0.688 | 0.355 |\n",
    "| bike | 1144 | 170 | 0.364 | 0.347 | 0.308 | 0.174 |\n",
    "| car | 1144 | 7128 | 0.74 | 0.725 | 0.781 | 0.527 |\n",
    "| motor | 1144 | 55 | 0.608 | 0.509 | 0.552 | 0.25 |\n",
    "| train | 1144 | 179 | 0.683 | 0.419 | 0.526 | 0.358 |\n",
    "| truck | 1144 | 2048 | 0.601 | 0.385 | 0.415 | 0.178 |\n",
    "| hydrant | 1144 | 94 | 0.687 | 0.149 | 0.274 | 0.147 |\n",
    "| sign | 1144 | 2472 | 0.608 | 0.313 | 0.362 | 0.195 |\n",
    "| skateboard | 1144 | 3 | 0 | 0 | 0 | 0 |\n",
    "| stroller | 1144 | 6 | 1 | 0 | 0 | 0 |\n",
    "| other vehicle | 1144 | 63 | 0.112 | 0.0635 | 0.0254 | 0.0151 |\n",
    "\n",
    "> _Speed: 0.3ms preprocess, 9.6ms inference, 0.0ms loss, 0.6ms postprocess per image_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3dd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.101 available 😃 Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mimgz\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['imgsz=640'].\n\n    Arguments received: ['yolo', '--f=/home/pride/.local/share/jupyter/runtime/kernel-v3effdcea9c3b64678857f4d2822ea406a95a7e2ac.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/work/AIMET/.env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[10], line 1\u001b[0m\n    results_ir_s = backbone_ir_small.train(data=dataset_ir, epochs=25, imgz=1408)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/work/AIMET/.env/lib/python3.10/site-packages/ultralytics/engine/model.py:329\u001b[0m in \u001b[1;35mtrain\u001b[0m\n    self.trainer = (trainer or self._smart_load('trainer'))(overrides=args, _callbacks=self.callbacks)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/work/AIMET/.env/lib/python3.10/site-packages/ultralytics/engine/trainer.py:83\u001b[0m in \u001b[1;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/work/AIMET/.env/lib/python3.10/site-packages/ultralytics/cfg/__init__.py:113\u001b[0m in \u001b[1;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/work/AIMET/.env/lib/python3.10/site-packages/ultralytics/cfg/__init__.py:203\u001b[0;36m in \u001b[0;35mcheck_dict_alignment\u001b[0;36m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mimgz\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['imgsz=640'].\n\n    Arguments received: ['yolo', '--f=/home/pride/.local/share/jupyter/runtime/kernel-v3effdcea9c3b64678857f4d2822ea406a95a7e2ac.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n\n    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n\n    5. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "results_ir_s = backbone_ir_small.train(data=dataset_ir, epochs=25, imgz=1408)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493a184-4b4f-455c-aae0-2d67c7d2d018",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_ir_small_results.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c0e0c-83fc-44e7-b0e8-a21a8d534f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 14))\n",
    "\n",
    "im_batch0_labels = plt.imread('./assets/backbone_ir_small_val_batch0_labels.webp')\n",
    "im_batch0_pred = plt.imread('./assets/backbone_ir_small_val_batch0_pred.webp')\n",
    "im_batch1_labels = plt.imread('./assets/backbone_ir_small_val_batch1_labels.webp')\n",
    "im_batch1_pred = plt.imread('./assets/backbone_ir_small_val_batch1_pred.webp')\n",
    "im_batch2_labels = plt.imread('./assets/backbone_ir_small_val_batch2_labels.webp')\n",
    "im_batch2_pred = plt.imread('./assets/backbone_ir_small_val_batch2_pred.webp')\n",
    "\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "plt.title('batch0_labels')\n",
    "plt.imshow(im_batch0_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 2)\n",
    "plt.title('batch1_labels')\n",
    "plt.imshow(im_batch1_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 3)\n",
    "plt.title('batch2_labels')\n",
    "plt.imshow(im_batch2_labels)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 4)\n",
    "plt.title('batch0_pred')\n",
    "plt.imshow(im_batch0_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 5)\n",
    "plt.title('batch1_pred')\n",
    "plt.imshow(im_batch1_pred)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(2, 3, 6)\n",
    "plt.title('batch2_pred')\n",
    "plt.imshow(im_batch2_pred)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c2b1b-cd54-48f8-9c14-be48ea044080",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6b9c4-1c0e-4526-987e-ab22b1e86f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "results_ir_s = backbone_ir_small.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cadd2c-51ba-4857-8683-edd40fef5067",
   "metadata": {},
   "source": [
    "| Class | Images | Instances | P | R | mAP50 | mAP50-95 |\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "| all | 1144 | 16688 | 0.555 | 0.322 | 0.358 | 0.2 |\n",
    "| person | 1144 | 4470 | 0.691 | 0.632 | 0.687 | 0.356 |\n",
    "| bike | 1144 | 170 | 0.369 | 0.353 | 0.309 | 0.174 |\n",
    "| car | 1144 | 7128 | 0.743 | 0.725 | 0.781 | 0.527 |\n",
    "| motor | 1144 | 55 | 0.593 | 0.504 | 0.551 | 0.251 |\n",
    "| train | 1144 | 179 | 0.683 | 0.419 | 0.527 | 0.361 |\n",
    "| truck | 1144 | 2048 | 0.608 | 0.386 | 0.418 | 0.178 |\n",
    "| hydrant | 1144 | 94 | 0.695 | 0.149 | 0.274 | 0.148 |\n",
    "| sign | 1144 | 2472 | 0.614 | 0.313 | 0.362 | 0.195 |\n",
    "| skateboard | 1144 | 3 | 0 | 0 | 0 | 0 |\n",
    "| stroller | 1144 | 6 | 1 | 0 | 0 | 0 |\n",
    "| other vehicle | 1144 | 63 | 0.112 | 0.0635 | 0.0254 | 0.0151 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61458be9-3197-4bea-8a59-e77d052ec3fd",
   "metadata": {},
   "source": [
    "![Training the YOLOv8 Model (RGB)](./assets/backbone_ir_small_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791fd55-9c79-4768-a22c-d6fb2a4c622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX format\n",
    "# success = backbone_nano.export(imgsz=(640, 480), format='onnx', opset=12, optimize=False, half=False)\n",
    "# Export to PyTorch format\n",
    "success = backbone_ir_small.export(imgsz=640, format='torchscript', optimize=False, half=False, int8=False)\n",
    "# TorchScript: export success ✅ 1.6s, saved as 'runs/detect/train6/weights/best.torchscript' (12.4 MB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
